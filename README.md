# Awesome-LLMs
收集整理开源LLMs，尤其是中文LLMs。持续更新中

## LLMs评价比较
TBD

## LLMs信息整理
* **Yuan-2.0** (2023.11.26)
   > 浪潮信息开源，支持2、51、102B。最大102B开源，提出了局部注意力过滤增强机制LFA
   - 仓库：[IEIT-Yuan/Yuan-2.0](https://github.com/IEIT-Yuan/Yuan-2.0)
   - 技术报告：[YUAN 2.0: A Large Language Model with Localized Filtering-based Attention](https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf)
* **Yi** (2023.11.02)
  > 零一万物开源，支持6、34B
  - 仓库：[01-ai/Yi](https://github.com/01-ai/Yi)
  - 技术报告：暂无
* **Baichuan2** (2023.09.06)
   > 百川智能开源，支持7、13B
   - 仓库：[baichuan-inc/Baichuan2](https://github.com/baichuan-inc/Baichuan2)
   - 技术报告：[Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)
* **Chinese-LLaMA-Alpaca** (2023.08.14)
  > 哈工大讯飞联合实验室开源，支持7、13、33B
  - 仓库：[Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
  - 技术报告：[Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca](https://arxiv.org/abs/2304.08177)
* **MiLM-6B** (2023.08.11)
   > 小米AI实验室大模型团队发布，支持1.3、6B，暂未开源
   - 仓库：[XiaoMi/MiLM-6B](https://github.com/XiaoMi/MiLM-6B)
   -  技术报告：暂无
* **Chinese LLaMA 2** (2023.07.28)
  > 华东师范大学发布，支持7B的LoRA微调
  - 仓库：[Chinese-LlaMA2](https://github.com/michael-wzhu/Chinese-LlaMA2)
  - 技术报告：无
* **LLaMA 2** (2023.07.19)
  > Meta开源，支持7、13、70B
  - 仓库：[facebookresearch/llama](https://github.com/facebookresearch/llama)
  - Recipe：[facebookresearch/llama-recipes](https://github.com/facebookresearch/llama-recipes)
  - 技术报告：[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
* **Baichuan** (2023.06.15)
  > 百川智能开源，支持7、13B
  - 仓库：[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B), [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B)
  - 技术报告：无
* **ChatGLM-6B** (2023.03.14)
  > 清华大学与智普AI联合开源，支持6B
  - 仓库：[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main)
  - 技术报告：无
* **LLaMa** (2023.02.25)
  > Meta开源，支持7、13、34、65B
  - 仓库：[llama_v1](https://github.com/facebookresearch/llama/tree/llama_v1)
  - 技术报告：[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)
